# Model Invocation Flow

## How the Currency Detection Model is Invoked

This document explains how `best_float32.tflite` is loaded and executed in the app.

### 1. Model Initialization (Startup)

**When:** App starts, DetectionScreen loads
**Where:** `lib/screens/detection_screen.dart` â†’ `_initializeCamera()`

```
_initializeCamera()
  â”œâ”€ Read: _detectionProvider = context.read<DetectionProvider>()
  â”œâ”€ Create: CameraController
  â”œâ”€ Initialize camera
  â”œâ”€ Log: "ğŸ“¹ Initializing currency detector model..."
  â”œâ”€ Call: await _detectionProvider.initialize()
  â”‚   â””â”€ Call: await detector.initialize()
  â”‚       â””â”€ Load: Interpreter.fromAsset('assets/models/best_float32.tflite')
  â”‚       â”‚   â””â”€ TensorFlow Lite loads the trained model into memory
  â”‚       â”‚   â””â”€ Model is ready for inference
  â”‚       â””â”€ Log: "âœ“ Currency detector model loaded successfully"
  â”œâ”€ Log: "âœ“ Currency detector ready!"
  â”œâ”€ Log: "ğŸ“¸ Starting camera frame stream..."
  â”œâ”€ Call: _cameraController.startImageStream()
  â”‚   â””â”€ Camera begins capturing frames
  â”‚   â””â”€ Each frame passed to callback
  â”œâ”€ Call: _detectionProvider.startDetection()
  â”‚   â””â”€ Set _isRunning = true
  â””â”€ Log: "â–¶ Live detection started"
```

### 2. Real-Time Frame Processing (~30 FPS)

**When:** Every camera frame (approximately 30 times per second)
**Where:** `lib/providers/detection_provider.dart` â†’ `processFrame()`

```
CameraFrame (from camera)
  â”‚
  â”œâ”€ startImageStream() callback receives CameraImage
  â”‚
  â”œâ”€ Call: _detectionProvider.processFrame(cameraImage)
  â”‚   â”‚
  â”‚   â”œâ”€ Check: if (!_isRunning || !_isInitialized) return
  â”‚   â”‚
  â”‚   â”œâ”€ Call: detector.detect(cameraImage)
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ STEP 1: Prepare Input Tensor
  â”‚   â”‚   â”‚   Call: _prepareInput(cameraImage)
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Create: List<List<List<List<double>>>> [1, 640, 640, 3]
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Get: camera dimensions (width, height)
  â”‚   â”‚   â”‚   â”œâ”€ Log: "ğŸ–¼ï¸  Camera frame: {width}x{height}, Format: {format}"
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Check: if (format == yuv420)
  â”‚   â”‚   â”‚   â”‚   â””â”€ Call: _fillInputFromYUV()
  â”‚   â”‚   â”‚   â”‚       â”œâ”€ Extract Y, U, V planes from camera frame
  â”‚   â”‚   â”‚   â”‚       â”œâ”€ Resize to 640x640
  â”‚   â”‚   â”‚   â”‚       â”œâ”€ Convert YUV to RGB using: _yuv420ToRgb()
  â”‚   â”‚   â”‚   â”‚       â””â”€ Normalize: r/255.0, g/255.0, b/255.0
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Check: else if (format == bgra8888)
  â”‚   â”‚   â”‚   â”‚   â””â”€ Call: _fillInputFromBGRA()
  â”‚   â”‚   â”‚   â”‚       â”œâ”€ Extract B, G, R, A bytes
  â”‚   â”‚   â”‚   â”‚       â”œâ”€ Resize to 640x640
  â”‚   â”‚   â”‚   â”‚       â””â”€ Normalize: r/255.0, g/255.0, b/255.0
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â””â”€ Log: "âœ“ Input tensor prepared: [1, 640, 640, 3]"
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ STEP 2: Run Model Inference
  â”‚   â”‚   â”‚   Call: _interpreter.run(input, output)
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Input:  [1, 640, 640, 3] normalized float tensor
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â””â”€ Output: [1, 25200, 85] raw predictions
  â”‚   â”‚   â”‚       â”œâ”€ 25200 = 80x80 + 40x40 + 20x20 grid cells
  â”‚   â”‚   â”‚       â””â”€ 85 = 4 box coords + 1 confidence + 80 class scores
  â”‚   â”‚   â”‚              (NOTE: Updated for 25 custom classes)
  â”‚   â”‚   â”‚
  â”‚   â”‚   â”œâ”€ STEP 3: Parse Output Detections
  â”‚   â”‚   â”‚   Call: _parseOutput(output[0])
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Log: "ğŸ“Š Parsing output - Type: {type}, Length: {length}"
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ Loop: for each of 25200 predictions
  â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ Extract: confidence = pred[4]
  â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ Check: if confidence >= 0.5 (confidenceThreshold)
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Extract: x, y, w, h = pred[0:4]
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Find: max class score in pred[5:30]
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€ classId = argmax(pred[5:30])
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Get: label = currencyClasses[classId]
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   Possible classes:
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - 100_back, 100_front
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - 200_back, 200_front
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - 500_back, 500_front
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - 50_back, 50_front
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - Gandhi_potrait
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - ashoka_piller
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - bleed_lines
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - color_chg_num
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   - ... (25 total)
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€ Create: CurrencyDetection {confidence, label, x, y, w, h}
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”œâ”€ STEP 4: Remove Duplicates with NMS
  â”‚   â”‚   â”‚   â”‚   Call: _applyNMS(detections)
  â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ Sort: by confidence (highest first)
  â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ Loop: for each detection
  â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Keep: detection with highest confidence
  â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Loop: compare with remaining detections
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€ Calculate: IoU (Intersection over Union)
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€ Suppress: if IoU > 0.5 (iouThreshold)
  â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â”‚   â””â”€ Return: filtered detections list
  â”‚   â”‚   â”‚   â”‚
  â”‚   â”‚   â”‚   â””â”€ Return: List<CurrencyDetection>
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€ Return: final detections
  â”‚   â”‚
  â”‚   â”œâ”€ Update: _detections = detections
  â”‚   â”‚
  â”‚   â”œâ”€ Check: if (detections.isNotEmpty)
  â”‚   â”‚   â”œâ”€ Log: "ğŸ¯ Detected {count} objects"
  â”‚   â”‚   â””â”€ For each detection:
  â”‚   â”‚       â””â”€ Log: "   - {label}: {confidence*100}%"
  â”‚   â”‚
  â”‚   â””â”€ Call: notifyListeners()
  â”‚       â””â”€ Trigger UI rebuild
  â”‚
  â””â”€ UI Update
      â”œâ”€ Consumer<DetectionProvider> rebuilds
      â”œâ”€ Get: detectionProvider.detections
      â””â”€ Call: DetectionPainter.paint()
          â””â”€ Draw bounding boxes on camera preview
```

### 3. Key Logging Points for Debugging

To verify the model is being invoked correctly, watch for these logs in the terminal:

**On App Start:**
```
ğŸ“¹ Initializing currency detector model...
âœ“ Currency detector model loaded successfully
âœ“ Currency detector ready!
ğŸ“¸ Starting camera frame stream...
â–¶ Live detection started
```

**On Each Frame:**
```
ğŸ–¼ï¸  Camera frame: 1080x2400, Format: ImageFormatGroup.yuv420
âœ“ Input tensor prepared: [1, 640, 640, 3]
ğŸ“Š Parsing output - Type: List<dynamic>, Length: 25200
ğŸ¯ Detected 3 objects
   - 500_back: 92.5%
   - 100_front: 87.3%
   - security_thread: 78.1%
```

**If Model Fails:**
```
âŒ Error during inference: {error message}
âŒ Error processing frame: {error message}
```

### 4. Model Configuration

File: `lib/services/yolo_detector.dart`

```dart
static const int inputSize = 640;           // Input tensor size
static const double confidenceThreshold = 0.5;  // Min confidence
static const double iouThreshold = 0.5;     // NMS threshold

static const List<String> currencyClasses = [
  '100_back', '100_front',      // â‚¹100 notes
  '200_back', '200_front',      // â‚¹200 notes
  '500_back', '500_front',      // â‚¹500 notes
  '50_back', '50_front',        // â‚¹50 notes
  'Gandhi_potrait',             // Security features
  'ashoka_piller',
  'bleed_lines',
  'color_chg_num',
  // ... 15 more features
];
```

### 5. Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| No logs at all | App not running | Ensure device is connected, `flutter run` succeeds |
| "Failed to load model" | Asset path wrong | Check `pubspec.yaml` has `assets/models/best_float32.tflite` |
| Model loaded but no detections | Confidence too high | Lower `confidenceThreshold` from 0.5 to 0.3 |
| Detections with wrong labels | Class name mismatch | Update `currencyClasses` to match trained model |
| Crash during inference | Input format mismatch | Check `_prepareInput()` handles camera format correctly |
| Black screen | Camera permission | Check Android manifest has camera permission |

### 6. Performance Notes

- **Initialization:** ~1-2 seconds (model loading)
- **Per-frame processing:** ~50-100ms (varies by device)
- **Camera FPS:** ~30 FPS on most devices
- **Detection latency:** Frames are processed asynchronously
- **Memory:** Model uses ~20-50MB depending on precision

### 7. Next Steps

1. **Run the app:** `flutter run -v`
2. **Watch the terminal** for initialization logs
3. **Point camera at currency notes** to trigger detections
4. **Check detected labels and confidence** match your trained model
5. **If needed, adjust thresholds** in `yolo_detector.dart`
